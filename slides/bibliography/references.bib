% ============================================================
% 1. Agent Foundations & Reviews (overview and definitions)
% ============================================================

@article{sapkota2025ai,
  title={AI agents vs. agentic AI: A conceptual taxonomy, applications and challenges},
  author={Sapkota, Ranjan and Roumeliotis, Konstantinos I and Karkee, Manoj},
  journal={arXiv preprint arXiv:2505.10468},
  year={2025}
}

@article{naveed2025comprehensive,
  title={A comprehensive overview of large language models},
  author={Naveed, H and Khan, AU and Qiu, S and Saqib, M and Anwar, S and Usman, M and Akhtar, N and Barnes, N and Mian, A},
  journal={ACM Transactions on Intelligent Systems and Technology},
  volume={16},
  number={5},
  pages={1--72},
  year={2025},
  publisher={ACM New York, NY},
  month={Aug}
}

@misc{weng2023prompt,
  title = {LLM Powered Autonomous Agents},
  author = {Weng, Lilian},
  year = {2023},
  month = {jun},
  howpublished = {lilianweng.github.io},
  url = {https://lilianweng.github.io/posts/2023-06-23-agent/},
  note = {Key reference for Agent Architecture (Memory, Planning, Tools).}
}

@inproceedings{park2023generative,
  title={Generative agents: Interactive simulacra of human behavior},
  author={Park, Joon Sung and O'Brien, Joseph C and Cai, Carrie J and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S},
  booktitle={Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
  pages={1--22},
  year={2023}
}

% ============================================================
% 2. LLM Foundations (model theory)
% ============================================================

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

% --- Architectures (BERT, T5, etc.) ---

@inproceedings{devlin2018bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of NAACL-HLT},
  year={2019},
  note={The BERT paper}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  note={The T5 paper}
}

% --- Decoding Strategies ---

@inproceedings{holtzman2019curious,
  title={The curious case of neural text degeneration},
  author={Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  booktitle={International Conference on Learning Representations},
  year={2020},
  note={Introduced Nucleus (Top-p) Sampling}
}

% --- Alignment (RLHF / RLAIF) ---

@inproceedings{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  booktitle={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017},
  note={Foundational RLHF paper}
}

@article{bai2022constitutional,
  title={Constitutional AI: Harmlessness from AI Feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022},
  note={The key paper for RLAIF and Constitutional AI}
}

@inproceedings{rafailov2023direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  booktitle={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024},
  note={DPO paper}
}

% --- Modern Models (Mistral, DeepSeek, Qwen) ---

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{deepseek2024deepseek,
  title={DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model},
  author={DeepSeek-AI},
  journal={arXiv preprint arXiv:2405.04434},
  year={2024}
}

@article{bai2023qwen,
  title={Qwen technical report},
  author={Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Ge, Yu and Han, Gao and others},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}

% ============================================================
% 3. Reasoning & Planning (agent reasoning patterns)
% ============================================================

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{yao2022react,
  title={ReAct: Synergizing reasoning and acting in language models},
  author={Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  journal={arXiv preprint arXiv:2210.03629},
  year={2022}
}


% --- Reasoning & Planning Extensions ---

@inproceedings{yao2023tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L and Cao, Yuan and Narasimhan, Karthik},
  booktitle={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023},
  note={Tree of Thoughts (ToT)}
}

@inproceedings{shinn2023reflexion,
  title={Reflexion: Language agents with verbal reinforcement learning},
  author={Shinn, Noah and Cassano, Federico and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu},
  booktitle={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023},
  note={Reflexion: Self-correction mechanism}
}

@article{wang2022self,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2022}
}

% --- RAG (Retrieval Augmented Generation) ---

@inproceedings{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020},
  note={The foundational RAG paper}
}

@article{gao2023retrieval,
  title={Retrieval-augmented generation for large language models: A survey},
  author={Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Haofen},
  journal={arXiv preprint arXiv:2312.10997},
  year={2023}
}

% ============================================================
% 4. Tool Use & MCP (tooling and connection standards)
% ============================================================

@article{schick2024toolformer,
  title={Toolformer: Language models can teach themselves to use tools},
  author={Schick, Timo and Dwivedi-Yu, Jane and Dess√¨, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{nakano2021webgpt,
  title={Webgpt: Browser-assisted question-answering with human feedback},
  author={Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and others},
  journal={arXiv preprint arXiv:2112.09332},
  year={2021}
}

% [ADDED] Essential for the MCP Section
@misc{anthropic2024mcp,
  title = {Model Context Protocol (MCP)},
  author = {{Anthropic}},
  year = {2024},
  howpublished = {\url{https://modelcontextprotocol.io/}},
  note = {The open standard for connecting AI models to data and tools.}
}

% [ADDED] Essential for the MCP Section
@misc{mcp_github,
  author = {{Model Context Protocol}},
  title = {Model Context Protocol Specifications},
  year = {2024},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/modelcontextprotocol}},
}

% ============================================================
% 5. Frameworks & Multi-Agent Systems (frameworks and multi-agent)
% ============================================================

@inproceedings{chase2022langchain,
  title={LangChain: Building applications with LLMs through composability},
  author={Chase, Harrison},
  year={2022},
  organization={GitHub}
}

% [ADDED] Essential for Multi-Agent Section
@article{wu2023autogen,
  title={Autogen: Enabling next-gen llm applications},
  author={Wu, Qingyun and Bansal, Gagan and Zhang, Jieyu and Wu, Yiran and Li, Beibin and Peng, Erkang and Walsh, Toby and Xiong, Chi},
  journal={arXiv preprint arXiv:2308.08155},
  year={2023}
}

% [ADDED] Essential for Multi-Agent Section
@inproceedings{li2023camel,
  title={CAMEL: Communicative Agents for "Mind" Exploration of Large Language Model Society},
  author={Li, Guohao and Hammoud, Hasan Abed Al Kader and Itani, Hani and Khizbullin, Dmitrii and Ghanem, Bernard},
  booktitle={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

% [ADDED] Essential for Frameworks Section
@misc{liu2022llamaindex,
  author = {Liu, Jerry},
  title = {LlamaIndex},
  year = {2022},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/run-llama/llama_index}},
}

% ============================================================
% 6. Notable Models (key models)
% ============================================================

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{anthropic2024claude,
  title={The Claude 3 Model Family: Opus, Sonnet, Haiku},
  author={Anthropic},
  journal={Anthropic Technical Report},
  year={2024}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}
